{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CS 3110/5110: Data Privacy\n",
    "## Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Plan\n",
    "\n",
    "My question is: Can I predict income levels (above thresholds or within categories), based on demographic features (age, education, etc.) in a privacy-preserving way? I plan on doing this by training a differentially private logistic regression model on census data using private gradient descent methods. I will also evaluate accuracy and privacy trade-offs by varying epsilon and clipping parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def laplace_mech_vec(vec, sensitivity, epsilon):\n",
    "    return [v + np.random.laplace(loc=0, scale=sensitivity / epsilon) for v in vec]\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n",
    "    return [v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "            for v in vec]\n",
    "\n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting and Setting up Datasets\n",
    "\n",
    "note:\n",
    "- There are some features which originally have 'N' as a possible value so I am removing those or changing it to 0 if 0 has not already been used\n",
    "\n",
    "- The columns with 'N' possible are: MSP, NOC, NPF, INDP_CAT, EDU, PINCP, PINCP_DECILE POVPIP, DVET, DREM, DPHY, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18, 1, '6', ..., 2, 72, 0],\n",
       "       [21, 2, '6', ..., 2, 6, 0],\n",
       "       [22, 2, '6', ..., 2, 80, 0],\n",
       "       ...,\n",
       "       [3, 1, '0', ..., 2, 69, 75],\n",
       "       [1, 2, '0', ..., 2, 64, 75],\n",
       "       [0, 1, '0', ..., 2, 107, 145]], dtype=object)"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset I am using - NIST data with income information for Massachusetts\n",
    "data = pd.read_csv('UCI Datasets/ma2019.csv')\n",
    "\n",
    "\n",
    "# The ones of these that already use 0: INDP_CAT, NOC   (so dropping these)\n",
    "data = data.drop(columns=['PUMA', 'NOC', 'INDP_CAT'])   #along with PUMA because its a string\n",
    "\n",
    "#Need to do some cleaning so all features are numeric and can be used in the model\n",
    "data.replace('N', '0', inplace=True)\n",
    "\n",
    "np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will add a column to the data that will be the target feature\n",
    "- This column will be binary 0 for bottom 50% of income\n",
    "- and 1 for top 50% of income\n",
    "\n",
    "This will allow logistic regression to perform the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PINCP_BIN\n",
       "0    4022\n",
       "1    3612\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adds new column to data\n",
    "data['PINCP_BIN'] = (data['PINCP_DECILE'].astype(int) >= 5).astype(int)\n",
    "#data['PINCP_BIN'] = np.where(data['PINCP_DECILE'].astype(int) >= 5, 1, -1)\n",
    "\n",
    "#checking to make sure it worked\n",
    "data['PINCP_BIN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (6107, 5) (6107,)\n",
      "Testing set shape: (1527, 5) (1527,)\n"
     ]
    }
   ],
   "source": [
    "#now defining the features that are model will be built off\n",
    "#Age, Sex, Marital Status, Race, and Education \n",
    "training_features = ['AGEP', 'SEX', 'MSP', 'RAC1P', 'EDU']\n",
    "X = data[training_features]\n",
    "\n",
    "#the goal of this project is to predict income percentile so income is our target variable\n",
    "#I made this a binary variable so people in the top half of income bracket have a score of 1\n",
    "# and people in the bottom half have a score of 0\n",
    "y = data['PINCP_BIN']\n",
    "\n",
    "#here is our split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "- I am using a logistic regression model to solve a multiclass classification problem of predicting income for people in this dataset\n",
    "- My target variable is income decile\n",
    "- My training features are age, sex, marital status, race, and education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [ 0.01212454 -1.03434411 -0.10248699 -0.0881801   0.51908816]\n",
      "Model accuracy: 0.7596594629993452\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    model = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "model = train_model()\n",
    "print('Model coefficients:', model.coef_[0])\n",
    "print('Model accuracy:', np.sum(model.predict(X_test) == y_test)/X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  Importance\n",
      "1     SEX    1.034344\n",
      "4     EDU    0.519088\n",
      "2     MSP    0.102487\n",
      "3   RAC1P    0.088180\n",
      "0    AGEP    0.012125\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names and coefficients\n",
    "X_train = pd.DataFrame(X_train, columns=training_features)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,  # Replace with your feature names\n",
    "    'Importance': np.abs(model.coef_[0])  # Absolute value of coefficients\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance)\n",
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Use gradient descent to find the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "# This is the logistic loss function.\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
    "def gradient(theta, xi, yi):\n",
    "    xi = np.array(xi).astype(int)\n",
    "    yi = np.array(yi).astype(int)  # Ensure yi is a scalar integer\n",
    "    \n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    exponent = np.clip(exponent, -700, 700)\n",
    "    return - (yi*xi) / (1+np.exp(exponent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = [0 for _ in range(5)] # I use 5 for the amount of training features I have\n",
    "i=1\n",
    "\n",
    "y_train = np.reshape(y_train,(y_train.size, 1))\n",
    "y_train.shape\n",
    "gradient(theta, X_train[i], y_train[i]).shape  # Use iloc to get the correct row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-12.09497298,  -0.34067464,  -0.56025872,  -0.3456689 ,\n",
       "        -1.99869003])"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_grad(theta, X, y):\n",
    "    #list of vectors, each vector has length 104\n",
    "    all_grads = [gradient(theta, X[i], y[i]) for i in range(len(X))]\n",
    "    #compute the column-wise average\n",
    "    avg_grad = np.mean(all_grads, axis=0)\n",
    "    return avg_grad\n",
    "\n",
    "avg_grad(theta, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(xi, theta, bias=0):\n",
    "    xi = np.array(xi).astype(int)\n",
    "    label = np.sign(xi @ theta + bias)\n",
    "    return label\n",
    "\n",
    "def accuracy(theta):\n",
    "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48526522593320237"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(iterations):\n",
    "    theta = [0 for _ in range(5)] #initial model\n",
    "    for _ in range(iterations):\n",
    "        theta = theta - avg_grad(theta, X_train, y_train)\n",
    "    return theta\n",
    "\n",
    "theta = gradient_descent(10)\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Noise\n",
    "\n",
    "So, we have successfully implemented the regression model that predicts income based on the data\n",
    "- Now we must add Differential Privacy to the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we must clip\n",
    "\n",
    "# L2 Clipping function we will use for our gradient descent\n",
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    \n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "    \n",
    "\n",
    "#Helper function for our noisy gradient descent\n",
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [L2_clip(gradient(theta, x_i, y_i), b) for x_i, y_i in zip(X,y)]\n",
    "        \n",
    "    # sum query\n",
    "    # L2 sensitivity is b (by clipping performed above)\n",
    "    return np.sum(gradients, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.44138834315651604\n"
     ]
    }
   ],
   "source": [
    "# Noisy gradient descent\n",
    "# Satisfies (k*epsilon + epsilon, k*delta)-differential privacy\n",
    "def noisy_gradient_descent(iterations, epsilon, delta):\n",
    "    theta = np.array([0 for _ in range(5)]) #resets theta to zeros\n",
    "    b = 20      #dont do 50 or more\n",
    "    epsilon_i = epsilon/iterations\n",
    "    delta_i = delta/iterations\n",
    "\n",
    "\n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, epsilon)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        clipped_gradient_sum = gradient_sum(theta, X_train, y_train, b)\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_vec(clipped_gradient_sum, b, epsilon_i, delta_i))\n",
    "        noisy_avg_gradient = np.array(noisy_gradient_sum) / len(X_train) #noisy_count\n",
    "        theta = theta - noisy_avg_gradient\n",
    "    return theta\n",
    "\n",
    "#New accuracy with noise added\n",
    "theta = noisy_gradient_descent(10, .01, 1e-5)\n",
    "print('Final accuracy:', accuracy(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Little Analysis\n",
    "\n",
    "- With a larger epsilon, I find that the addition of noise does not change the accuracy of the model at all\n",
    "- The more I decrease the epsilon, the lower the accuracy gets. This is a good sign that the noise is successfully being added and that tradeoff is appearing of less accuracy with more noise.\n",
    "- My clipping parameter is relatively small because I adjusted for the change in the size of data I was working with. My dataset is not small, but it it not as large as the adult set and does not require as much clipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Methods\n",
    "\n",
    "I wanted to try out some of the more advanced Differential Privacy methods as well and see how the model performs with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mech_RDP_vec(vec, sensitivity, alpha, epsilon):\n",
    "    sigma = np.sqrt((sensitivity**2 * alpha) / (2 * epsilon))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.22291664  0.7774238   7.32252653 -0.60986016 11.28356649]\n",
      "Final accuracy: 0.48526522593320237\n"
     ]
    }
   ],
   "source": [
    "def noisy_gradient_descent_RDP(iterations, alpha, epsilon_bar):\n",
    "    theta = np.array([0 for _ in range(5)]) #resets theta to zeros\n",
    "    b = 20   #or b\n",
    "    epsilon_i = epsilon_bar/iterations\n",
    "    alpha_i = alpha/iterations\n",
    "\n",
    "    for i in range(iterations):\n",
    "        #gradient sum:\n",
    "        # 1. computes the gradients for all of X_train\n",
    "        # 2. clips them\n",
    "        # 3. and sums them\n",
    "        grad_sum  = gradient_sum(theta, X_train, y_train, b) \n",
    "\n",
    "        #now we add noise with renyi diff priv\n",
    "        noisy_grad_sum = gaussian_mech_RDP_vec(grad_sum, sensitivity=b, alpha=alpha_i, epsilon=epsilon_i)\n",
    "\n",
    "        noisy_grad = np.array(noisy_grad_sum)/len(X_train)\n",
    "        theta = theta - noisy_grad\n",
    "    return theta\n",
    "\n",
    "theta = noisy_gradient_descent_RDP(10, 20, 0.000001)\n",
    "print(theta)\n",
    "print('Final accuracy:', accuracy(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.48526522593320237\n"
     ]
    }
   ],
   "source": [
    "def noisy_gradient_descent_zCDP(iterations, rho):\n",
    "    theta = np.array([0 for _ in range(5)]) #resets theta to zeros\n",
    "    b = 20\n",
    "    rho_i = rho/iterations\n",
    "\n",
    "    for i in range(iterations):\n",
    "        #gradient sum:\n",
    "        # 1. computes the gradients for all of X_train\n",
    "        # 2. clips them\n",
    "        # 3. and sums them\n",
    "        grad_sum  = gradient_sum(theta, X_train, y_train, b) \n",
    "\n",
    "        #now we add noise with renyi diff priv\n",
    "        noisy_grad_sum = gaussian_mech_zCDP_vec(grad_sum, sensitivity=b, rho=rho_i)\n",
    "\n",
    "        noisy_grad = np.array(noisy_grad_sum)/len(X_train)\n",
    "        theta = theta - noisy_grad\n",
    "    return theta\n",
    "\n",
    "theta = noisy_gradient_descent_zCDP(10, 0.001)\n",
    "print('Final accuracy:', accuracy(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.48526522593320237\n"
     ]
    }
   ],
   "source": [
    "def noisy_gradient_descent_zCDP_ED(iterations, epsilon, delta):\n",
    "    theta = np.array([0 for _ in range(5)]) #resets theta to zeros\n",
    "    b = 20\n",
    "    tolerance = 0.1 * epsilon\n",
    "    \n",
    "    #convert epsilon and delta to rho\n",
    "\n",
    "    rho_low, rho_high = 0, 1  # initial bounds for rho\n",
    "    while rho_high - rho_low > tolerance:\n",
    "        rho_mid = (rho_high + rho_low) / 2\n",
    "        # Calculate epsilon for this rho using zCDP to DP conversion formula\n",
    "        calculated_epsilon = np.sqrt(2 * rho_mid * np.log(1 / delta)) + rho_mid\n",
    "        if calculated_epsilon < epsilon:\n",
    "            rho_low = rho_mid\n",
    "        else:\n",
    "            rho_high = rho_mid\n",
    "    \n",
    "    rho = (rho_high + rho_low) / 2\n",
    "    rho_i = rho/iterations\n",
    "\n",
    "\n",
    "    for i in range(iterations):\n",
    "        #gradient sum:\n",
    "        # 1. computes the gradients for all of X_train\n",
    "        # 2. clips them\n",
    "        # 3. and sums them\n",
    "        grad_sum  = gradient_sum(theta, X_train, y_train, b) \n",
    "\n",
    "        #now we add noise with renyi diff priv\n",
    "        noisy_grad_sum = gaussian_mech_zCDP_vec(grad_sum, sensitivity=b, rho=rho_i)\n",
    "\n",
    "        noisy_grad = np.array(noisy_grad_sum)/len(X_train)\n",
    "        theta = theta - noisy_grad\n",
    "    return theta\n",
    "\n",
    "\n",
    "\n",
    "theta = noisy_gradient_descent_zCDP_ED(10, .0001, 1e-5)\n",
    "print('Final accuracy:', accuracy(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Analysis\n",
    "\n",
    "These methods basically all show similar solutions, but just test the same model out with a different differential privacy algorithms.\n",
    "\n",
    "The model I have developed is not the most accurate. It seems to only be able to predict what income bracket a person is in a little less than 50% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
