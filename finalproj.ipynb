{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CS 3110/5110: Data Privacy\n",
    "## Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Plan\n",
    "\n",
    "My question is: Can I predict income levels (above thresholds or within categories), based on demographic features (age, education, etc.) in a privacy-preserving way? I plan on doing this by training a differentially private logistic regression model on census data using private gradient descent methods. I will also evaluate accuracy and privacy trade-offs by varying epsilon and clipping parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def laplace_mech_vec(vec, sensitivity, epsilon):\n",
    "    return [v + np.random.laplace(loc=0, scale=sensitivity / epsilon) for v in vec]\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n",
    "    return [v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "            for v in vec]\n",
    "\n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting and Setting up Datasets\n",
    "\n",
    "note:\n",
    "- There are some features which originally have 'N' as a possible value so I am removing those or changing it to 0 if 0 has not already been used\n",
    "\n",
    "- The columns with 'N' possible are: MSP, NOC, NPF, INDP_CAT, EDU, PINCP, PINCP_DECILE POVPIP, DVET, DREM, DPHY, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18, 1, '6', ..., 2, 72, 0],\n",
       "       [21, 2, '6', ..., 2, 6, 0],\n",
       "       [22, 2, '6', ..., 2, 80, 0],\n",
       "       ...,\n",
       "       [3, 1, '0', ..., 2, 69, 75],\n",
       "       [1, 2, '0', ..., 2, 64, 75],\n",
       "       [0, 1, '0', ..., 2, 107, 145]], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset I am using - NIST data with income information for Massachusetts\n",
    "data = pd.read_csv('NIST Datasets/ma2019.csv')\n",
    "\n",
    "\n",
    "# The ones of these that already use 0: INDP_CAT, NOC   (so dropping these)\n",
    "data = data.drop(columns=['PUMA', 'NOC', 'INDP_CAT'])   #along with PUMA because its a string\n",
    "\n",
    "#Need to do some cleaning so all features are numeric and can be used in the model\n",
    "data.replace('N', '0', inplace=True)\n",
    "\n",
    "np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will add a column to the data that will be the target feature\n",
    "- This column will be binary 0 for bottom 50% of income\n",
    "- and 1 for top 50% of income\n",
    "\n",
    "This will allow logistic regression to perform the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PINCP_BIN\n",
       "-1    4022\n",
       " 1    3612\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adds new column to data\n",
    "data['PINCP_BIN'] = (data['PINCP_DECILE'].astype(int) >= 5).astype(int)\n",
    "#makes it 1 and -1 instead of 0 and 1\n",
    "data['PINCP_BIN'] = np.where(data['PINCP_DECILE'].astype(int) >= 5, 1, -1)\n",
    "\n",
    "#checking to make sure it worked\n",
    "data['PINCP_BIN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (6107, 31) (6107,)\n",
      "Testing set shape: (1527, 31) (1527,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  1, ..., -1, -1, -1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now defining the features that are model will be built off\n",
    "#Age, Sex, Marital Status, Race, and Education \n",
    "training_features = ['AGEP', 'SEX', 'MSP', 'RAC1P', 'EDU']\n",
    "df = data.copy()\n",
    "df = df[training_features]\n",
    "df = df.astype(int)\n",
    "#normalizes age columns\n",
    "df['AGEP'] = df['AGEP']/df['AGEP'].sum()\n",
    "#this one-hot encodes all the columns\n",
    "for col in training_features:\n",
    "    if col == 'AGEP':\n",
    "        continue\n",
    "    # Get one hot encoding of columns B\n",
    "    one_hot = pd.get_dummies(df[col], prefix=col)\n",
    "    # Drop column B as it is now encoded\n",
    "    df = df.drop(col,axis = 1)\n",
    "    # Join the encoded df\n",
    "    df = df.join(one_hot)\n",
    "\n",
    "X = df\n",
    "\n",
    "#the goal of this project is to predict income percentile so income is our target variable\n",
    "#I made this a binary variable so people in the top half of income bracket have a score of 1\n",
    "# and people in the bottom half have a score of -1\n",
    "y = data['PINCP_BIN']\n",
    "\n",
    "#here is our split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "- I am using a logistic regression model to solve a multiclass classification problem of predicting income for people in this dataset\n",
    "- My target variable is income decile\n",
    "- My training features are age, sex, marital status, race, and education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [-2.47266291e-03  3.32929717e-01 -7.01340715e-01 -3.64423617e+00\n",
      "  9.22760148e-01  4.84246029e-02  3.64992465e-01  9.25788547e-01\n",
      "  1.11769026e+00 -1.03830851e-01  3.93530933e-01  1.58505680e-01\n",
      " -5.45844934e-01 -4.48631827e-01 -2.33396588e-01 -1.23754298e-01\n",
      "  5.88021305e-02  3.72377905e-01 -9.05027012e-01 -1.14840691e+00\n",
      " -1.01869129e+00 -1.50757213e+00 -1.75810546e+00 -2.81027026e-01\n",
      " -4.74937264e-01 -9.18022666e-02  5.47155507e-01  1.15431229e+00\n",
      "  1.65582964e+00  1.65868174e+00  1.80117919e+00]\n",
      "Model accuracy: 0.77079240340537\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    model = LogisticRegression( solver='lbfgs', max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "model = train_model()\n",
    "print('Model coefficients:', model.coef_[0])\n",
    "print('Model accuracy:', np.sum(model.predict(X_test) == y_test)/X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Use gradient descent to find the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
    "# This is the logistic loss function.\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))\n",
    "\n",
    "# This is the gradient of the logistic loss\n",
    "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
    "def gradient(theta, xi, yi):\n",
    "    xi = np.array(xi).astype(int)\n",
    "    yi = np.array(yi).astype(int)  # Ensure yi is a scalar integer\n",
    "    \n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    exponent = np.clip(exponent, -700, 700)\n",
    "    return - (yi*xi) / (1+np.exp(exponent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [0 for _ in range(X_train.shape[1])] # I use 5 for the amount of training features I have\n",
    "i=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00, -2.48075978e-02,  5.46913378e-02,  7.40953005e-02,\n",
       "       -8.35107254e-02,  1.39184542e-03,  7.53233994e-03, -7.12297364e-03,\n",
       "       -1.14622564e-03,  3.86441788e-02,  1.25266088e-02,  4.42115605e-03,\n",
       "        2.45619781e-04,  8.18732602e-05,  6.95922712e-03,  8.18732602e-05,\n",
       "        1.39184542e-03,  4.17553627e-03,  1.30997216e-02,  8.84231210e-03,\n",
       "        1.36728345e-02,  4.37203209e-02,  2.57082037e-02,  1.76027509e-02,\n",
       "        1.63746520e-03,  1.35090879e-02, -5.07614213e-03, -4.84689700e-02,\n",
       "       -3.79073195e-02, -7.69608646e-03, -8.76043884e-03])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_grad(theta, X, y):\n",
    "    #list of vectors, each vector has length 104\n",
    "    all_grads = [gradient(theta, X[i], y[i]) for i in range(len(X))]\n",
    "    #compute the column-wise average\n",
    "    avg_grad = np.mean(all_grads, axis=0)\n",
    "    return avg_grad\n",
    "\n",
    "avg_grad(theta, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
    "def predict(xi, theta, bias=0):\n",
    "    xi = np.array(xi).astype(int)\n",
    "    label = np.sign(xi @ theta + bias)\n",
    "    return label\n",
    "\n",
    "def accuracy(theta):\n",
    "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75049115913556"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(iterations):\n",
    "    theta = [0 for _ in range(X_train.shape[1])] #initial model\n",
    "    for _ in range(iterations):\n",
    "        theta = theta - avg_grad(theta, X_train, y_train)\n",
    "    return theta\n",
    "\n",
    "theta = gradient_descent(10)\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Noise\n",
    "\n",
    "So, we have successfully implemented the regression model that predicts income based on the data\n",
    "- Now we must add Differential Privacy to the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we must clip\n",
    "\n",
    "# L2 Clipping function we will use for our gradient descent\n",
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    \n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v\n",
    "    \n",
    "\n",
    "#Helper function for our noisy gradient descent\n",
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [L2_clip(gradient(theta, x_i, y_i), b) for x_i, y_i in zip(X,y)]\n",
    "        \n",
    "    # sum query\n",
    "    # L2 sensitivity is b (by clipping performed above)\n",
    "    return np.sum(gradients, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.4119187950229208\n"
     ]
    }
   ],
   "source": [
    "# Noisy gradient descent\n",
    "# Satisfies (k*epsilon + epsilon, k*delta)-differential privacy\n",
    "def noisy_gradient_descent(iterations, epsilon, delta):\n",
    "    theta = np.array([0 for _ in range(X_train.shape[1])]) #resets theta to zeros\n",
    "    b = 20      #dont do 50 or more\n",
    "    epsilon_i = epsilon/iterations\n",
    "    delta_i = delta/iterations\n",
    "\n",
    "\n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, epsilon)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        clipped_gradient_sum = gradient_sum(theta, X_train, y_train, b)\n",
    "        noisy_gradient_sum = np.array(gaussian_mech_vec(clipped_gradient_sum, b, epsilon_i, delta_i))\n",
    "        noisy_avg_gradient = np.array(noisy_gradient_sum) / noisy_count\n",
    "        theta = theta - noisy_avg_gradient\n",
    "    return theta\n",
    "\n",
    "#New accuracy with noise added\n",
    "theta = noisy_gradient_descent(10, .1, 1e-5)\n",
    "print('Final accuracy:', accuracy(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Little Analysis\n",
    "\n",
    "- With a larger epsilon, I find that the addition of noise does not change the accuracy of the model at all\n",
    "- The more I decrease the epsilon, the lower the accuracy gets. This is a good sign that the noise is successfully being added and that tradeoff is appearing of less accuracy with more noise.\n",
    "- My clipping parameter is relatively small because I adjusted for the change in the size of data I was working with. My dataset is not small, but it it not as large as the adult set and does not require as much clipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Methods\n",
    "\n",
    "I wanted to try out some of the more advanced Differential Privacy methods as well and see how the model performs with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mech_RDP_vec(vec, sensitivity, alpha, epsilon):\n",
    "    sigma = np.sqrt((sensitivity**2 * alpha) / (2 * epsilon))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01831659  0.09504225 -0.38509286 -0.40219744  0.65987646  0.12139743\n",
      " -0.0393608   0.03881423 -0.0598912  -0.28006535 -0.13514719 -0.12958657\n",
      " -0.13962637 -0.02391976 -0.01251062  0.00275001  0.01778892  0.1213913\n",
      "  0.06335248 -0.10040623 -0.17161267 -0.36904156 -0.01463687 -0.10685879\n",
      "  0.04193414 -0.14037366 -0.01873506  0.42306843  0.288214    0.04043388\n",
      "  0.07210003]\n",
      "Final accuracy: 0.7098886705959397\n"
     ]
    }
   ],
   "source": [
    "def noisy_gradient_descent_RDP(iterations, alpha, epsilon_bar):\n",
    "    theta = np.array([0 for _ in range(X_train.shape[1])]) #resets theta to zeros\n",
    "    b = 20   #or b\n",
    "    epsilon_i = epsilon_bar/iterations\n",
    "    alpha_i = alpha/iterations\n",
    "\n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, epsilon_bar)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        #gradient sum:\n",
    "        # 1. computes the gradients for all of X_train\n",
    "        # 2. clips them/\n",
    "        # 3. and sums them\n",
    "        grad_sum  = gradient_sum(theta, X_train, y_train, b) \n",
    "\n",
    "        #now we add noise with renyi diff priv\n",
    "        noisy_grad_sum = gaussian_mech_RDP_vec(grad_sum, sensitivity=b, alpha=alpha_i, epsilon=epsilon_i)\n",
    "\n",
    "        noisy_grad = np.array(noisy_grad_sum)/noisy_count  \n",
    "        theta = theta - noisy_grad\n",
    "    return theta\n",
    "\n",
    "theta = noisy_gradient_descent_RDP(10, 20, .1)\n",
    "print(theta)\n",
    "print('Final accuracy:', accuracy(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
    "    sigma = np.sqrt((sensitivity**2) / (2 * rho))\n",
    "    return [v + np.random.normal(loc=0, scale=sigma) for v in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.739358218729535\n"
     ]
    }
   ],
   "source": [
    "def noisy_gradient_descent_zCDP(iterations, rho):\n",
    "    theta = np.array([0 for _ in range(X_train.shape[1])]) #resets theta to zeros\n",
    "    b = 20\n",
    "    rho_i = rho/iterations\n",
    "\n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, rho)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        #gradient sum:\n",
    "        # 1. computes the gradients for all of X_train\n",
    "        # 2. clips them\n",
    "        # 3. and sums them\n",
    "        grad_sum  = gradient_sum(theta, X_train, y_train, b) \n",
    "\n",
    "        #now we add noise with renyi diff priv\n",
    "        noisy_grad_sum = gaussian_mech_zCDP_vec(grad_sum, sensitivity=b, rho=rho_i)\n",
    "\n",
    "        noisy_grad = np.array(noisy_grad_sum)/noisy_count\n",
    "        theta = theta - noisy_grad\n",
    "    return theta\n",
    "\n",
    "theta = noisy_gradient_descent_zCDP(10, .1)\n",
    "print('Final accuracy:', accuracy(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.7007203667321545\n"
     ]
    }
   ],
   "source": [
    "def noisy_gradient_descent_zCDP_ED(iterations, epsilon, delta):\n",
    "    theta = np.array([0 for _ in range(X_train.shape[1])]) #resets theta to zeros\n",
    "    b = 20\n",
    "    tolerance = 0.1 * epsilon\n",
    "    \n",
    "    #convert epsilon and delta to rho\n",
    "\n",
    "    rho_low, rho_high = 0, 1  # initial bounds for rho\n",
    "    while rho_high - rho_low > tolerance:\n",
    "        rho_mid = (rho_high + rho_low) / 2\n",
    "        # Calculate epsilon for this rho using zCDP to DP conversion formula\n",
    "        calculated_epsilon = np.sqrt(2 * rho_mid * np.log(1 / delta)) + rho_mid\n",
    "        if calculated_epsilon < epsilon:\n",
    "            rho_low = rho_mid\n",
    "        else:\n",
    "            rho_high = rho_mid\n",
    "    \n",
    "    rho = (rho_high + rho_low) / 2\n",
    "    rho_i = rho/iterations\n",
    "\n",
    "\n",
    "    for i in range(iterations):\n",
    "        #gradient sum:\n",
    "        # 1. computes the gradients for all of X_train\n",
    "        # 2. clips them\n",
    "        # 3. and sums them\n",
    "        grad_sum  = gradient_sum(theta, X_train, y_train, b) \n",
    "\n",
    "        #now we add noise with renyi diff priv\n",
    "        noisy_grad_sum = gaussian_mech_zCDP_vec(grad_sum, sensitivity=b, rho=rho_i)\n",
    "\n",
    "        noisy_grad = np.array(noisy_grad_sum)/len(X_train)\n",
    "        theta = theta - noisy_grad\n",
    "    return theta\n",
    "\n",
    "\n",
    "\n",
    "theta = noisy_gradient_descent_zCDP_ED(10, .1, 1e-5)\n",
    "print('Final accuracy:', accuracy(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Analysis\n",
    "\n",
    "These methods basically all show similar solutions, but just test the same model out with a different differential privacy algorithm.\n",
    "\n",
    "Tested with 10 iterations, .1 epsilon, b=20, and delta of 1e-5\n",
    "\n",
    "The models have varying accuracies:\n",
    "- scikit learn logistic regression - 77% accurate\n",
    "\n",
    "- pure gradient descent - 75% accurate\n",
    "\n",
    "- gradient descent with noise - roughly 50% accurate(40-60% range)\n",
    "\n",
    "- Renyi-Differentially Private Gradient Descent - roughly 70% accurate\n",
    "\n",
    "- zero-Concentrated differential privacy gradient descent - roughly 74% accurate\n",
    "\n",
    "- zCDP to Epsilon, Delta gradient descent - roughly 65-70% accurate\n",
    "    - Above algorithm seems to vary more: probably due to the nature of its algorithm and guessing rho"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
